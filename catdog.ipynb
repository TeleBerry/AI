{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------準備資料--------------------------------------------------\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm_notebook\n",
    "from random import shuffle\n",
    "import shutil\n",
    "import pandas as pd\n",
    "# ---------準備資料--------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------建立引數--------------------------------------------------\n",
    "def organize_datasrts(path_to_data, n=4000, ratio=0.2):\n",
    "    files = os.listdir(path_to_data)\n",
    "    files-[os.path.join(path_to_data, f)for f in files]\n",
    "    shuffle(files)\n",
    "    files = files[:n]\n",
    "\n",
    "    n = int(len(files)*ratio)\n",
    "    val, train = files[:n], files[n:]\n",
    "\n",
    "    shutil.rmtree('./data/')\n",
    "    print('/data/ removed')\n",
    "\n",
    "    for c in ['dog', 'cat']:\n",
    "        os.makedirs('./data/train/{0}/'.format(c))\n",
    "        os.makedirs('./data/validation/{0}/'.format(c))\n",
    "\n",
    "    print('folders created !')\n",
    "\n",
    "    for t in tqdm_notebook(train):\n",
    "        if 'cat' in t:\n",
    "            shutil.copy2(t, os.path.join('.', 'data', 'train', 'cats'))\n",
    "        else:\n",
    "            shutil.copy2(t, os.path.join('.', 'data', 'train', 'dogs'))\n",
    "\n",
    "    for v in tqdm_notebook(val):\n",
    "        if 'cat' in v:\n",
    "            shutil.copy2(v, os.path.join('.', 'data', 'validation', 'cats'))\n",
    "        else:\n",
    "            shutil.copy2(v, os.path.join('.', 'data', 'validation', 'dogs'))\n",
    "\n",
    "    print('data copied !')\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "#標準化處理\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1/255., #rescale放縮因子，調整像素\n",
    "    shear_range=0.2,  #錯切變換，讓座標保持不變\n",
    "    horizontal_flip=True #水平翻轉\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1/255.)\n",
    "\n",
    "# ---------建立引數--------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 images belonging to 2 classes.\n",
      "Found 20000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator=train_datagen.flow_from_directory('./data/train/',\n",
    "\t\t\t\t\t\t\t\t\ttarget_size=(150,150),batch_size=batch_size,class_mode='categorical')\n",
    "\n",
    "validation_generator=val_datagen.flow_from_directory('./data/train/',\n",
    "\t\t\t\t\t\t\t\t\ttarget_size=(150,150),batch_size=batch_size,class_mode='categorical')\n",
    "\n",
    "# Found 20000 images belonging to 2 classes.\n",
    "# Found 5000 images belonging to 2 classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 卷積/池化層(3)---------------------------------------------------------------------------------------------------\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(150, 150, 3), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# 全連接層(2)--------------------------------------------------------------\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 隨機梯度下降法-----------------------------------------------------------------------\n",
    "epochs = 50\n",
    "lrate = 0.01\n",
    "decay = lrate/epochs\n",
    "sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
    "model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-49-e2a8c0f8e103>, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-49-e2a8c0f8e103>\"\u001b[1;36m, line \u001b[1;32m15\u001b[0m\n\u001b[1;33m    (monitor='val_loss',min_delta=0,patience=2,verbose=0,mode='auto')\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# Callback for loss logging per ephoch\n",
    "\n",
    "class LossHistory(Callback):\n",
    "\tdef on_train_begin(self,logs={}):\n",
    "\t\t\tself.losses=[]\n",
    "\t\t\tself.val_losses=[]\n",
    "\tdef on_epoch_end(self,batch,logs={}):\n",
    "\t\t\tself.losses.append(logs.get('loss'))\n",
    "\t\t\tself.val_losses.append(logs.get('val_loss'))\n",
    "\n",
    "history=LossHistory()\n",
    "\n",
    "# Callback for early stopping the training\n",
    "early_stopping-keras.callbacks.EarlyStopping\n",
    "\t\t\t\t\t\t\t(monitor='val_loss',min_delta=0,patience=2,verbose=0,mode='auto') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
